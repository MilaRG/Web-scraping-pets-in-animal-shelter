{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Animals in Shelter Web Scraper**\n",
    "\n",
    "\n",
    "\n",
    "To scrape the data given the website i've choosen, the process will have 5 main sections:\n",
    "\n",
    "1. Preliminar evaluation\n",
    "2. Reach to the website \n",
    "3. Get url from each pet's site\n",
    "4. Scrape data from each pet's site\n",
    "5. Gather all data on csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installation of required packages and libraries for the project\n",
    "\n",
    "!pip install selenium\n",
    "!pip install python-whois\n",
    "!pip install tqdm\n",
    "!pip install requests-html\n",
    "import builtwith\n",
    "import whois\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preliminar evaluation**\n",
    "\n",
    "Previous evaluation, we will inspect Robots.txt, the technology used to build the website and find out the website owner, i'm using 'benestaranimal site' from 'Ajuntament de Barcelona', this site is both in spanish and catalan this is defined by the \"ca\" or \"es\" after the \"/\"at the end of the site. To evaluate Robots.txt i will have to use the roo url  meaning i have to reach https://ajuntament.barcelona.cat/benestaranimal/robots.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "print (\"technology used to develope the website i'm gonna use\")\n",
    "print(builtwith.builtwith('https://ajuntament.barcelona.cat/benestaranimal/ca'))\n",
    "print (\"owner of the website\")\n",
    "print(whois.whois('https://ajuntament.barcelona.cat/benestaranimal/ca'))\n",
    "print('robots.txt detail')\n",
    "print(requests.get('https://ajuntament.barcelona.cat/benestaranimal/robots.txt', data=None).text)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reach to website**\n",
    "\n",
    "Will have as a result that we can open the website with webdriver in selenium to navigate and get to the specific page where all data from the different animals is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url=\"https://ajuntament.barcelona.cat/benestaranimal/ca\"\n",
    "\n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "driver_setting=webdriver.ChromeOptions()\n",
    "driver_setting.add_argument(\"user-agent=Study_purpose_crawler\")\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "#i'm putting some delay between clicking buttons just to make sure the page is already loaded.\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"main-menu-ul\"]/li[4]/span').click()\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"main-menu-ul\"]/li[4]/div/ul/li[2]/ul/li[2]/ul/li[3]/a').click()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Get url from each pet's site**\n",
    "\n",
    "\n",
    "Get all the links from each pet's site on the website since the data that we want to scrape is not on the main search page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pets_links=[]\n",
    "condition=True\n",
    "while condition:\n",
    "    pets= driver.find_elements_by_css_selector('.views-row')\n",
    "    time.sleep(10)\n",
    "    for pet  in pets:\n",
    "        pets1=pet.find_element_by_class_name('field-content')\n",
    "        pet2=pets1.find_element_by_tag_name('a')\n",
    "        pets_links.append(pet2.get_property('href'))\n",
    "    try:\n",
    "                \n",
    "        driver.find_element_by_xpath(\"//a[contains(text(),'›')]\").click()\n",
    "        time.sleep(10)\n",
    "    except:\n",
    "        condition=False\n",
    "\n",
    "# check with len function if we have got more links.\n",
    "\n",
    "len(pets_links)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scrape data for each pet**\n",
    "\n",
    "\n",
    "Get the data from each pet on the shelter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alldata=[]\n",
    "\n",
    "for pet_link in tqdm(pets_links):\n",
    "    driver.get(pet_link)\n",
    "    time.sleep(random.uniform(4.0,5.0))\n",
    "    \n",
    "    name= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[1]/h2').text\n",
    "    animal_type= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[1]').text\n",
    "    race= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[2]').text\n",
    "    cross_race=driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[last()-4]').text\n",
    "    sex= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[last()-3]').text\n",
    "    age= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[last()-2]').text\n",
    "    weight= driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[last()-1]').text\n",
    "    id_animal=driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[2]/div[2]/div[last()]').text\n",
    "    link_picture=driver.find_element_by_xpath('//*[contains(@id,\"node-\")]/div[2]/div/div[1]/div/div[2]/div/img').get_attribute('src')\n",
    "    data= {'name':name,'Type':animal_type,'race':race,'cross_race':cross_race,'sex':sex,'age':age,'weight':weight,'id':id_animal,'link_picture':link_picture,'link_pet_site':pet_link}\n",
    "    alldata.append(data)\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gather all data on csv**\n",
    "\n",
    "\n",
    "Gather all the data on a dataframe and convert to csv, and print result to check if the result is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame(alldata)\n",
    "\n",
    "#Save as a csv:\n",
    "df.to_csv (r'C:\\Users\\mila_\\Documents\\Master ciencia de dades\\Tipología y ciclo de vida de los datos\\PRAC 1\\pets_in_shelter.csv', index = False, header=True)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
